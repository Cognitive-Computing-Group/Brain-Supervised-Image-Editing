{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "informed-persian",
   "metadata": {},
   "source": [
    "# Brain-Supervised Image Editing\n",
    "## Supplementary Material\n",
    "### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-plate",
   "metadata": {},
   "source": [
    "# EEG Data Prep - To Be Deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "experimental-sister",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import mne\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import re\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "#from neuromancer import utils\n",
    "epodir = \"../data/EEG_New/data/cleaned/\"\n",
    "from functools import reduce\n",
    "import matplotlib\n",
    "matplotlib.use('qt5agg') # Replace with appropriate backend\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import pandas as pd\n",
    "from os import listdir, path\n",
    "\n",
    "TASK_LIST = ['facecat/female','facecat/male',\n",
    "             'facecat/nosmile', 'facecat/smiles',\n",
    "             'facecat/old', 'facecat/young',\n",
    "            'facecat/blond', 'facecat/darkhaired']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-retro",
   "metadata": {},
   "source": [
    "# Load epochs and drop EOG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "polished-andrew",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../data/EEG_New/data/cleaned/P01-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3342 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P02-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3444 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P03-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3203 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P04-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3116 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P05-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3342 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P06-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3323 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P07-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3343 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P08-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3283 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P09-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3393 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P10-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3354 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P11-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3414 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P12-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "2767 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P13-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3302 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P14-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3259 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P15-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3406 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P16-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3329 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P17-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3309 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P18-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3292 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P19-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "2931 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P20-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3179 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P21-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3369 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P22-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3376 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P23-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3059 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P24-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3308 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P25-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3396 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P26-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "2874 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P27-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3426 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P28-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3111 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P29-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3310 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Reading ../data/EEG_New/data/cleaned/P30-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     900.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 3 columns\n",
      "Replacing existing metadata with 3 columns\n",
      "3277 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "epos = []\n",
    "for f in sorted(listdir(epodir)):\n",
    "    epo = mne.read_epochs(path.join(epodir, f))\n",
    "    # Drop the EOG channels\n",
    "    epos.append(epo.drop_channels([\"HEOG\", \"VEOG\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dense-weekly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(253, 32, 551)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epos[0]['facecat/blond'].get_data().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "temporal-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_to_lda_data(epoch, times=[0, 0.5]):\n",
    "    \n",
    "    \n",
    "    # Set up task dictionary to identify test/train splits\n",
    "    \n",
    "    task_dict = {'facecat/blond':['facecat/female','facecat/male', \n",
    "                                  'facecat/nosmile', 'facecat/old', \n",
    "                                  'facecat/smiles','facecat/young'],\n",
    "                 'facecat/darkhaired':['facecat/female', 'facecat/male', \n",
    "                                       'facecat/nosmile', 'facecat/old', \n",
    "                                        'facecat/smiles','facecat/young'], \n",
    "                 'facecat/female': ['facecat/blond','facecat/darkhaired',\n",
    "                                    'facecat/nosmile', 'facecat/old', \n",
    "                                    'facecat/smiles','facecat/young'],\n",
    "                 'facecat/male':['facecat/blond','facecat/darkhaired',\n",
    "                                 'facecat/nosmile', 'facecat/old', \n",
    "                                 'facecat/smiles','facecat/young'], \n",
    "                 'facecat/nosmile':['facecat/blond','facecat/darkhaired',\n",
    "                                     'facecat/female', 'facecat/male',\n",
    "                                     'facecat/old','facecat/young'],\n",
    "                 'facecat/old':['facecat/blond','facecat/darkhaired',\n",
    "         'facecat/female', 'facecat/male', 'facecat/nosmile', \n",
    "         'facecat/smiles'],\n",
    "                 'facecat/smiles':['facecat/blond','facecat/darkhaired',\n",
    "         'facecat/female', 'facecat/male', 'facecat/old', 'facecat/young'],\n",
    "                 'facecat/young':['facecat/blond','facecat/darkhaired',\n",
    "         'facecat/female', 'facecat/male', 'facecat/nosmile',\n",
    "         'facecat/smiles']}\n",
    "    \n",
    "    \n",
    "    # Get time indexes for slicing\n",
    "    tr_times = epoch.time_as_index([times[0], times[1]])\n",
    "    \n",
    "    lda_data = {}\n",
    "    \n",
    "    # Create train/test splits\n",
    "    for task in task_dict:\n",
    "        \n",
    "        train_x = []\n",
    "        train_y = []\n",
    "        \n",
    "        for other_task in task_dict[task]:\n",
    "            train_x.extend(np.array(epoch[other_task].get_data())[:,:,tr_times[0]:tr_times[1]])\n",
    "            train_y.append(epoch[other_task].metadata)\n",
    "        \n",
    "        lda_data[task] = {\"test_x\": np.array(epoch[task].get_data())[:,:,tr_times[0]:tr_times[1]] , \n",
    "                  \"test_y\": epoch[task].metadata.reset_index(drop=True), \n",
    "                  \"train_x\": np.array(train_x), \n",
    "                  \"train_y\": pd.concat(train_y).reset_index(drop=True)}\n",
    "    \n",
    "    return lda_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "destroyed-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "    task_dict = {'facecat/blond':['facecat/female','facecat/male', \n",
    "                                  'facecat/nosmile', 'facecat/old', \n",
    "                                  'facecat/smiles','facecat/young'],\n",
    "                 'facecat/darkhaired':['facecat/female', 'facecat/male', \n",
    "                                       'facecat/nosmile', 'facecat/old', \n",
    "                                        'facecat/smiles','facecat/young'], \n",
    "                 'facecat/female': ['facecat/blond','facecat/darkhaired',\n",
    "                                    'facecat/nosmile', 'facecat/old', \n",
    "                                    'facecat/smiles','facecat/young'],\n",
    "                 'facecat/male':['facecat/blond','facecat/darkhaired',\n",
    "                                 'facecat/nosmile', 'facecat/old', \n",
    "                                 'facecat/smiles','facecat/young'], \n",
    "                 'facecat/nosmile':['facecat/blond','facecat/darkhaired',\n",
    "                                     'facecat/female', 'facecat/male',\n",
    "                                     'facecat/old','facecat/young'],\n",
    "                 'facecat/old':['facecat/blond','facecat/darkhaired',\n",
    "         'facecat/female', 'facecat/male', 'facecat/nosmile', \n",
    "         'facecat/smiles'],\n",
    "                 'facecat/smiles':['facecat/blond','facecat/darkhaired',\n",
    "         'facecat/female', 'facecat/male', 'facecat/old', 'facecat/young'],\n",
    "                 'facecat/young':['facecat/blond','facecat/darkhaired',\n",
    "         'facecat/female', 'facecat/male', 'facecat/nosmile',\n",
    "         'facecat/smiles']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "copyrighted-lender",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['facecat/blond',\n",
       " 'facecat/darkhaired',\n",
       " 'facecat/female',\n",
       " 'facecat/male',\n",
       " 'facecat/nosmile',\n",
       " 'facecat/old',\n",
       " 'facecat/smiles',\n",
       " 'facecat/young']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(task_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adequate-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_average(df, n_split=15):\n",
    "    \"\"\"\n",
    "    Take epoch data and average into N_Splits,\n",
    "    n_split:  The SMALLER the number, the lower resolution the data is\n",
    "    \"\"\"\n",
    "    splits = np.split(df, n_split, axis=2)\n",
    "    mean_list = []\n",
    "    for i in splits:\n",
    "        mean_list.append(np.mean(i, axis=2))\n",
    "    \n",
    "    return(np.stack(mean_list, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "meaning-christianity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf9418e71aa423b8bc6f211d2fdf88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_data = dict()\n",
    "for i in tqdm(range(30)):\n",
    "    output_data[i] = dict()\n",
    "    for task in tqdm(list(task_dict.keys()), leave=False):\n",
    "        output_data[i][task] = dict()\n",
    "        lda_data = epoch_to_lda_data(epos[i], times=[0, 0.5])\n",
    "        try:\n",
    "            test_x = split_average(lda_data[task]['test_x'], n_split=5)\n",
    "            train_x = split_average(lda_data[task]['train_x'], n_split=5)\n",
    "            \n",
    "            output_data[i][task]['test'] = test_x\n",
    "            output_data[i][task]['train'] = train_x\n",
    "        except:\n",
    "            pass\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "juvenile-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lzma\n",
    "# open a file, where you ant to store the data\n",
    "file = lzma.open('facecat.xz', 'wb')\n",
    "\n",
    "# dump information to that file\n",
    "pickle.dump(output_data, file)\n",
    "\n",
    "# close the file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_data = epoch_to_lda_data(epos[2], times=[0, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = split_average(lda_data['facecat/blond']['test_x'], n_split=5)\n",
    "train_x = split_average(lda_data['facecat/blond']['train_x'], n_split=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-component",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a file, where you ant to store the data\n",
    "file = open('facecat.pkl', 'wb')\n",
    "\n",
    "# dump information to that file\n",
    "pickle.dump(lda_data, file)\n",
    "\n",
    "# close the file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = lda_data['facecat/blond']['test_y'].rel\n",
    "train_y = lda_data['facecat/blond']['train_y'].rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-cleveland",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reshape([train_x.shape[0], train_x.shape[1] * train_x.shape[2]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(solver = 'lsqr', shrinkage = 'auto')\n",
    "lda = lda.fit(train_x, list(train_y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_x.reshape([test_x.shape[0], test_x.shape[1] * test_x.shape[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-ballot",
   "metadata": {},
   "source": [
    "# Reshape Data so it is anonymized (100ms averages), with channel names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_example = epos[0]['facecat/blond']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "epos[0].ch_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "epos[0]['facecat/blond'].pick_channels(['Fp1']).get_data().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "epos[0]['facecat/blond'].get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-despite",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = lda.predict_proba(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.roc_auc_score(list(test_y), yhat[:,1], average=\"macro\", multi_class='ovo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Results Loop\n",
    "results_dict = {}\n",
    "for user in tqdm(range(len(epos))):\n",
    "    results_dict[user] = {}\n",
    "    lda_data = epoch_to_lda_data(epos[user], times=[-0.2, 0.9])\n",
    "    \n",
    "    for task in TASK_LIST:\n",
    "        test_x = split_average(lda_data[task]['test_x'], n_split=25)\n",
    "        train_x = split_average(lda_data[task]['train_x'], n_split=25)\n",
    "        \n",
    "        train_x = train_x.reshape([train_x.shape[0], train_x.shape[1] * train_x.shape[2]])\n",
    "\n",
    "        test_y = lda_data[task]['test_y'].rel\n",
    "        train_y = lda_data[task]['train_y'].rel\n",
    "        \n",
    "        img_ids = lda_data[task]['test_y'].img\n",
    "        event_type =  lda_data[task]['test_y'].event\n",
    "        \n",
    "        lda = LDA(solver = 'lsqr', shrinkage = 'auto')\n",
    "        lda = lda.fit(train_x, list(train_y)) \n",
    "        \n",
    "        test_x = test_x.reshape([test_x.shape[0], test_x.shape[1] * test_x.shape[2]])\n",
    "        yhat = lda.predict_proba(test_x)[:,1]\n",
    "        \n",
    "        results_dict[user][task] = {\"yhat\":yhat, \"y\":test_y, \"img_id\": list(img_ids), \"event\":event_type}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-billy",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.roc_auc_score(list(results_dict[0]['facecat/female']['y']), results_dict[0]['facecat/female']['yhat'], average=\"macro\", multi_class='ovo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Results Loop\n",
    "loresresults_dict = {}\n",
    "for user in tqdm(range(len(epos))):\n",
    "    loresresults_dict[user] = {}\n",
    "    lda_data = epoch_to_lda_data(epos[user], times=[0, 0.5])\n",
    "    \n",
    "    for task in TASK_LIST:\n",
    "        test_x = split_average(lda_data[task]['test_x'], n_split=5)\n",
    "        train_x = split_average(lda_data[task]['train_x'], n_split=5)\n",
    "        \n",
    "        train_x = train_x.reshape([train_x.shape[0], train_x.shape[1] * train_x.shape[2]])\n",
    "\n",
    "        test_y = lda_data[task]['test_y'].rel\n",
    "        train_y = lda_data[task]['train_y'].rel\n",
    "        \n",
    "        img_ids = lda_data[task]['test_y'].img\n",
    "        event_type =  lda_data[task]['test_y'].event\n",
    "        \n",
    "        lda = LDA(solver = 'lsqr', shrinkage = 'auto')\n",
    "        lda = lda.fit(train_x, list(train_y)) \n",
    "        \n",
    "        test_x = test_x.reshape([test_x.shape[0], test_x.shape[1] * test_x.shape[2]])\n",
    "        yhat = lda.predict_proba(test_x)[:,1]\n",
    "        \n",
    "        loresresults_dict[user][task] = {\"yhat\":yhat, \"y\":test_y, \"img_id\": list(img_ids), \"event\":event_type}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.roc_auc_score(list(loresresults_dict[0]['facecat/female']['y']), loresresults_dict[0]['facecat/female']['yhat'], average=\"macro\", multi_class='ovo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-narrative",
   "metadata": {},
   "source": [
    "# EEG Data - 100ms Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-marriage",
   "metadata": {},
   "source": [
    "# Stimuli Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-buying",
   "metadata": {},
   "source": [
    "# SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-dealer",
   "metadata": {},
   "source": [
    "# GAN Warping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-treasurer",
   "metadata": {},
   "source": [
    "# Github Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-valentine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-civilization",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-colonial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-significance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
